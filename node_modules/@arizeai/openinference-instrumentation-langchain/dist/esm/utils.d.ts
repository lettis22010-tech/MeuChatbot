import { Attributes } from "@opentelemetry/api";
import { OpenInferenceSpanKind } from "@arizeai/openinference-semantic-conventions";
import { Run } from "@langchain/core/tracers/base";
import { LLMMessagesAttributes, LLMParameterAttributes, PromptTemplateAttributes, RetrievalDocument, TokenCountAttributes, ToolAttributes } from "./types.js";
export declare const RETRIEVAL_DOCUMENTS: "retrieval.documents";
export declare const SESSION_ID_KEYS: readonly ["session_id", "thread_id", "conversation_id"];
/**
 * Flattens a nested object into a single level object with keys as dot-separated paths.
 * Specifies elements in arrays with their index as part of the path.
 * @param attributes - Nested attributes to flatten.
 * @param baseKey - Base key to prepend to all keys.
 * @returns Flattened attributes
 */
declare function flattenAttributes(attributes: Record<string, unknown>, baseKey?: string): Attributes;
/**
 * Gets the OpenInferenceSpanKind based on the langchain run type.
 * @param runType - The langchain run type
 * @returns The OpenInferenceSpanKind based on the langchain run type or "UNKNOWN".
 */
declare function getOpenInferenceSpanKindFromRunType(runType: string): OpenInferenceSpanKind;
/**
 * Formats the input or output of a langchain run into OpenInference attributes for a span.
 * @param ioConfig - The input or output of a langchain run and the type of IO
 * @param ioConfig.io - The input or output of a langchain run
 * @param ioConfig.ioType - The type of IO
 * @returns The formatted input or output attributes for the span
 */
declare function formatIO({ io, ioType, }: {
    io: Run["inputs"] | Run["outputs"];
    ioType: "input" | "output";
}): {
    [x: string]: string | null;
};
/**
 * Formats the input messages of a langchain run into OpenInference attributes.
 * @param input - The input of a langchain run.
 * @returns The OpenInference attributes for the input messages.
 */
declare function formatInputMessages(input: Run["inputs"]): LLMMessagesAttributes | null;
/**
 * Formats the output messages of a langchain run into OpenInference attributes.
 * @param output - The output of a langchain run.
 * @returns The OpenInference attributes for the output messages.
 */
declare function formatOutputMessages(output: Run["outputs"]): LLMMessagesAttributes | null;
/**
 * Formats the retrieval documents of a langchain run into OpenInference attributes.
 * @param run - The langchain run to extract the retrieval documents from
 * @returns The OpenInference attributes for the retrieval documents.
 */
declare function formatRetrievalDocuments(run: Run): {
    "retrieval.documents": RetrievalDocument[];
} | null;
/**
 * Gets the model name from the langchain run extra data.
 * @param runExtra - The extra data from a langchain run
 * @returns The OpenInference attributes for the model name
 */
declare function formatLLMParams(runExtra: Run["extra"]): LLMParameterAttributes | null;
/**
 * A best effort function to extract the prompt template from a langchain run.
 * @param run - The langchain run to extract the prompt template from
 * @returns The OpenInference attributes for the prompt template
 */
declare function formatPromptTemplate(run: Run): PromptTemplateAttributes | null;
/**
 * Formats the token counts of a langchain run into OpenInference attributes.
 * @param outputs - The outputs of a langchain run
 * @returns The OpenInference attributes for the token counts
 *
 * @see https://github.com/langchain-ai/langchainjs/blob/main/langchain-core/src/language_models/chat_models.ts#L403 for how token counts get added to outputs
 */
declare function formatTokenCounts(outputs: Run["outputs"]): TokenCountAttributes | null;
/**
 * Formats the function calls of a langchain run into OpenInference attributes.
 * @param outputs - The outputs of a langchain run
 * @returns The OpenInference attributes for the function calls
 */
declare function formatFunctionCalls(outputs: Run["outputs"]): {
    "llm.function_call": string | null;
} | null;
/**
 * Formats the tool calls of a langchain run into OpenInference attributes.
 * @param run - The langchain run to extract the tool calls from
 * @returns The OpenInference attributes for the tool calls
 */
declare function formatToolCalls(run: Run): ToolAttributes | null;
/**
 * Formats the metadata of a langchain run into OpenInference attributes.
 * @param run - The langchain run to extract the metadata from
 * @returns The OpenInference attributes for the metadata
 */
declare function formatMetadata(run: Run): {
    metadata: string | null;
} | null;
/**
 * Formats the session id of a langchain run into OpenInference attributes.
 *
 * @see https://docs.smith.langchain.com/observability/how_to_guides/monitoring/threads#group-traces-into-threads
 *
 * @param run - The langchain run to extract the session id from
 * @returns The OpenInference attributes for the session id
 */
declare function formatSessionId(run: Run): {
    "session.id": unknown;
} | null;
export declare const safelyFlattenAttributes: import("@arizeai/openinference-core").SafeFunction<typeof flattenAttributes>;
export declare const safelyFormatIO: import("@arizeai/openinference-core").SafeFunction<typeof formatIO>;
export declare const safelyFormatInputMessages: import("@arizeai/openinference-core").SafeFunction<typeof formatInputMessages>;
export declare const safelyFormatOutputMessages: import("@arizeai/openinference-core").SafeFunction<typeof formatOutputMessages>;
export declare const safelyGetOpenInferenceSpanKindFromRunType: import("@arizeai/openinference-core").SafeFunction<typeof getOpenInferenceSpanKindFromRunType>;
export declare const safelyFormatRetrievalDocuments: import("@arizeai/openinference-core").SafeFunction<typeof formatRetrievalDocuments>;
export declare const safelyFormatLLMParams: import("@arizeai/openinference-core").SafeFunction<typeof formatLLMParams>;
export declare const safelyFormatPromptTemplate: import("@arizeai/openinference-core").SafeFunction<typeof formatPromptTemplate>;
export declare const safelyFormatTokenCounts: import("@arizeai/openinference-core").SafeFunction<typeof formatTokenCounts>;
export declare const safelyFormatFunctionCalls: import("@arizeai/openinference-core").SafeFunction<typeof formatFunctionCalls>;
export declare const safelyFormatToolCalls: import("@arizeai/openinference-core").SafeFunction<typeof formatToolCalls>;
export declare const safelyFormatMetadata: import("@arizeai/openinference-core").SafeFunction<typeof formatMetadata>;
export declare const safelyFormatSessionId: import("@arizeai/openinference-core").SafeFunction<typeof formatSessionId>;
export {};
//# sourceMappingURL=utils.d.ts.map