"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.safelyFormatSessionId = exports.safelyFormatMetadata = exports.safelyFormatToolCalls = exports.safelyFormatFunctionCalls = exports.safelyFormatTokenCounts = exports.safelyFormatPromptTemplate = exports.safelyFormatLLMParams = exports.safelyFormatRetrievalDocuments = exports.safelyGetOpenInferenceSpanKindFromRunType = exports.safelyFormatOutputMessages = exports.safelyFormatInputMessages = exports.safelyFormatIO = exports.safelyFlattenAttributes = exports.SESSION_ID_KEYS = exports.RETRIEVAL_DOCUMENTS = void 0;
const api_1 = require("@opentelemetry/api");
const typeUtils_1 = require("./typeUtils");
const core_1 = require("@opentelemetry/core");
const openinference_semantic_conventions_1 = require("@arizeai/openinference-semantic-conventions");
const openinference_core_1 = require("@arizeai/openinference-core");
exports.RETRIEVAL_DOCUMENTS = `${openinference_semantic_conventions_1.SemanticAttributePrefixes.retrieval}.${openinference_semantic_conventions_1.RetrievalAttributePostfixes.documents}`;
exports.SESSION_ID_KEYS = [
    "session_id",
    "thread_id",
    "conversation_id",
];
/**
 * Handler for any unexpected errors that occur during processing.
 */
const onError = (message) => (error) => {
    api_1.diag.warn(`OpenInference-LangChain: error processing langchain run, falling back to null. ${message}. ${error}`);
};
const safelyJSONStringify = (0, openinference_core_1.withSafety)({
    fn: JSON.stringify,
    onError: onError("Error stringifying JSON"),
});
/**
 * Flattens a nested object into a single level object with keys as dot-separated paths.
 * Specifies elements in arrays with their index as part of the path.
 * @param attributes - Nested attributes to flatten.
 * @param baseKey - Base key to prepend to all keys.
 * @returns Flattened attributes
 */
function flattenAttributes(attributes, baseKey = "") {
    const result = {};
    for (const key in attributes) {
        const newKey = baseKey ? `${baseKey}.${key}` : key;
        const value = attributes[key];
        if (value == null) {
            continue;
        }
        if ((0, typeUtils_1.isObject)(value)) {
            Object.assign(result, flattenAttributes(value, newKey));
        }
        else if (Array.isArray(value)) {
            value.forEach((item, index) => {
                if ((0, typeUtils_1.isObject)(item)) {
                    Object.assign(result, flattenAttributes(item, `${newKey}.${index}`));
                }
                else {
                    result[`${newKey}.${index}`] = item;
                }
            });
        }
        else if ((0, core_1.isAttributeValue)(value)) {
            result[newKey] = value;
        }
    }
    return result;
}
/**
 * Gets the OpenInferenceSpanKind based on the langchain run type.
 * @param runType - The langchain run type
 * @returns The OpenInferenceSpanKind based on the langchain run type or "UNKNOWN".
 */
function getOpenInferenceSpanKindFromRunType(runType) {
    const normalizedRunType = runType.toUpperCase();
    if (normalizedRunType.includes("AGENT")) {
        return openinference_semantic_conventions_1.OpenInferenceSpanKind.AGENT;
    }
    if (normalizedRunType in openinference_semantic_conventions_1.OpenInferenceSpanKind) {
        return openinference_semantic_conventions_1.OpenInferenceSpanKind[normalizedRunType];
    }
    return openinference_semantic_conventions_1.OpenInferenceSpanKind.CHAIN;
}
/**
 * Formats the input or output of a langchain run into OpenInference attributes for a span.
 * @param ioConfig - The input or output of a langchain run and the type of IO
 * @param ioConfig.io - The input or output of a langchain run
 * @param ioConfig.ioType - The type of IO
 * @returns The formatted input or output attributes for the span
 */
function formatIO({ io, ioType, }) {
    let valueAttribute;
    let mimeTypeAttribute;
    switch (ioType) {
        case "input": {
            valueAttribute = openinference_semantic_conventions_1.SemanticConventions.INPUT_VALUE;
            mimeTypeAttribute = openinference_semantic_conventions_1.SemanticConventions.INPUT_MIME_TYPE;
            break;
        }
        case "output": {
            valueAttribute = openinference_semantic_conventions_1.SemanticConventions.OUTPUT_VALUE;
            mimeTypeAttribute = openinference_semantic_conventions_1.SemanticConventions.OUTPUT_MIME_TYPE;
            break;
        }
        default:
            (0, typeUtils_1.assertUnreachable)(ioType);
    }
    if (io == null) {
        return {};
    }
    const values = Object.values(io);
    if (values.length === 1 && typeof values[0] === "string") {
        return {
            [valueAttribute]: values[0],
            [mimeTypeAttribute]: openinference_semantic_conventions_1.MimeType.TEXT,
        };
    }
    return {
        [valueAttribute]: safelyJSONStringify(io),
        [mimeTypeAttribute]: openinference_semantic_conventions_1.MimeType.JSON,
    };
}
/**
 * Gets the role of a message from the langchain message data.
 * @param messageData - The langchain message data to extract the role from
 * @returns The role of the message or null
 */
function getRoleFromMessageData(messageData) {
    const messageIds = messageData.lc_id;
    if (!(0, typeUtils_1.isNonEmptyArray)(messageIds)) {
        return null;
    }
    const langchainMessageClass = messageIds[messageIds.length - 1];
    const normalizedLangchainMessageClass = (0, typeUtils_1.isString)(langchainMessageClass)
        ? langchainMessageClass.toLowerCase()
        : "";
    if (normalizedLangchainMessageClass.includes("human")) {
        return "user";
    }
    if (normalizedLangchainMessageClass.includes("ai")) {
        return "assistant";
    }
    if (normalizedLangchainMessageClass.includes("system")) {
        return "system";
    }
    if (normalizedLangchainMessageClass.includes("function")) {
        return "function";
    }
    if (normalizedLangchainMessageClass.includes("chat") &&
        (0, typeUtils_1.isObject)(messageData.kwargs) &&
        (0, typeUtils_1.isString)(messageData.kwargs.role)) {
        return messageData.kwargs.role;
    }
    return null;
}
/**
 * Gets the content of a message from the langchain message kwargs.
 * @param messageKwargs - The langchain message kwargs to extract the content from
 * @returns The content of the message or null
 */
function getContentFromMessageData(messageKwargs) {
    return (0, typeUtils_1.isString)(messageKwargs.content) ? messageKwargs.content : null;
}
function getFunctionCallDataFromAdditionalKwargs(additionalKwargs) {
    const functionCall = additionalKwargs.function_call;
    if (!(0, typeUtils_1.isObject)(functionCall)) {
        return {};
    }
    const functionCallName = (0, typeUtils_1.isString)(functionCall.name)
        ? functionCall.name
        : undefined;
    const functionCallArgs = (0, typeUtils_1.isString)(functionCall.args)
        ? functionCall.args
        : undefined;
    return {
        [openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_NAME]: functionCallName,
        [openinference_semantic_conventions_1.SemanticConventions.MESSAGE_FUNCTION_CALL_ARGUMENTS_JSON]: functionCallArgs,
    };
}
/**
 * Gets the tool calls from the langchain message additional kwargs and formats them into OpenInference attributes.
 * @param additionalKwargs - The langchain message additional kwargs to extract the tool calls from
 * @returns the OpenInference attributes for the tool calls
 */
function getToolCallDataFromAdditionalKwargs(additionalKwargs) {
    const toolCalls = additionalKwargs.tool_calls;
    if (!Array.isArray(toolCalls)) {
        return {};
    }
    const formattedToolCalls = toolCalls.map((toolCall) => {
        if (!(0, typeUtils_1.isObject)(toolCall) && !(0, typeUtils_1.isObject)(toolCall.function)) {
            return {};
        }
        const toolCallName = (0, typeUtils_1.isString)(toolCall.function.name)
            ? toolCall.function.name
            : undefined;
        const toolCallArgs = (0, typeUtils_1.isString)(toolCall.function.arguments)
            ? toolCall.function.arguments
            : undefined;
        return {
            [openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_NAME]: toolCallName,
            [openinference_semantic_conventions_1.SemanticConventions.TOOL_CALL_FUNCTION_ARGUMENTS_JSON]: toolCallArgs,
        };
    });
    return {
        [openinference_semantic_conventions_1.SemanticConventions.MESSAGE_TOOL_CALLS]: formattedToolCalls,
    };
}
/**
 * Parses a langchain message into OpenInference attributes.
 * @param messageData - The langchain message data to parse
 * @returns The OpenInference attributes for the message
 */
function parseMessage(messageData) {
    const message = {};
    const maybeRole = getRoleFromMessageData(messageData);
    if (maybeRole != null) {
        message[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_ROLE] = maybeRole;
    }
    const messageKwargs = messageData.lc_kwargs;
    if (!(0, typeUtils_1.isObject)(messageKwargs)) {
        return message;
    }
    const maybeContent = getContentFromMessageData(messageKwargs);
    if (maybeContent != null) {
        message[openinference_semantic_conventions_1.SemanticConventions.MESSAGE_CONTENT] = maybeContent;
    }
    const additionalKwargs = messageKwargs.additional_kwargs;
    if (!(0, typeUtils_1.isObject)(additionalKwargs)) {
        return message;
    }
    return Object.assign(Object.assign(Object.assign({}, message), getFunctionCallDataFromAdditionalKwargs(additionalKwargs)), getToolCallDataFromAdditionalKwargs(additionalKwargs));
}
/**
 * Formats the input messages of a langchain run into OpenInference attributes.
 * @param input - The input of a langchain run.
 * @returns The OpenInference attributes for the input messages.
 */
function formatInputMessages(input) {
    const maybeMessages = input.messages;
    if (!(0, typeUtils_1.isNonEmptyArray)(maybeMessages)) {
        return null;
    }
    // Only support the first 'set' of messages
    const firstMessages = maybeMessages[0];
    if (!(0, typeUtils_1.isNonEmptyArray)(firstMessages)) {
        return null;
    }
    const parsedMessages = [];
    firstMessages.forEach((messageData) => {
        if (!(0, typeUtils_1.isObject)(messageData)) {
            return;
        }
        parsedMessages.push(parseMessage(messageData));
    });
    if (parsedMessages.length > 0) {
        return { [openinference_semantic_conventions_1.SemanticConventions.LLM_INPUT_MESSAGES]: parsedMessages };
    }
    return null;
}
/**
 * Gets the first generation of the output of a langchain run.
 * @param output - The output of a langchain run.
 * @returns The first generation of the output or null.
 */
function getFirstOutputGeneration(output) {
    if (!(0, typeUtils_1.isObject)(output)) {
        return null;
    }
    const maybeGenerations = output.generations;
    if (!(0, typeUtils_1.isNonEmptyArray)(maybeGenerations)) {
        return null;
    }
    // Only support the first 'set' of generations
    const firstGeneration = maybeGenerations[0];
    if (!(0, typeUtils_1.isNonEmptyArray)(firstGeneration)) {
        return null;
    }
    return firstGeneration;
}
/**
 * Formats the output messages of a langchain run into OpenInference attributes.
 * @param output - The output of a langchain run.
 * @returns The OpenInference attributes for the output messages.
 */
function formatOutputMessages(output) {
    const firstGeneration = getFirstOutputGeneration(output);
    if (firstGeneration == null) {
        return null;
    }
    const parsedMessages = [];
    firstGeneration.forEach((generation) => {
        if (!(0, typeUtils_1.isObject)(generation) || !(0, typeUtils_1.isObject)(generation.message)) {
            return;
        }
        parsedMessages.push(parseMessage(generation.message));
    });
    if (parsedMessages.length > 0) {
        return { [openinference_semantic_conventions_1.SemanticConventions.LLM_OUTPUT_MESSAGES]: parsedMessages };
    }
    return null;
}
/**
 * Parses a langchain retrieval document into OpenInference attributes.
 * @param document - The langchain retrieval document to parse
 * @returns The OpenInference attributes for the retrieval document
 */
function parseRetrievalDocument(document) {
    var _a;
    if (!(0, typeUtils_1.isObject)(document)) {
        return null;
    }
    const parsedDocument = {};
    if ((0, typeUtils_1.isString)(document.pageContent)) {
        parsedDocument["document.content"] = document.pageContent;
    }
    if ((0, typeUtils_1.isObject)(document.metadata)) {
        parsedDocument["document.metadata"] =
            (_a = safelyJSONStringify(document.metadata)) !== null && _a !== void 0 ? _a : undefined;
    }
    return parsedDocument;
}
/**
 * Formats the retrieval documents of a langchain run into OpenInference attributes.
 * @param run - The langchain run to extract the retrieval documents from
 * @returns The OpenInference attributes for the retrieval documents.
 */
function formatRetrievalDocuments(run) {
    const normalizedRunType = run.run_type.toLowerCase();
    if (normalizedRunType !== "retriever") {
        return null;
    }
    if (!(0, typeUtils_1.isObject)(run.outputs) || !Array.isArray(run.outputs.documents)) {
        return null;
    }
    return {
        [exports.RETRIEVAL_DOCUMENTS]: run.outputs.documents
            .map(parseRetrievalDocument)
            .filter((doc) => doc != null),
    };
}
/**
 * Gets the model name from the langchain run extra data.
 * @param runExtra - The extra data from a langchain run
 * @returns The OpenInference attributes for the model name
 */
function formatLLMParams(runExtra) {
    var _a;
    if (!(0, typeUtils_1.isObject)(runExtra) || !(0, typeUtils_1.isObject)(runExtra.invocation_params)) {
        return null;
    }
    const openInferenceParams = {};
    openInferenceParams[openinference_semantic_conventions_1.SemanticConventions.LLM_INVOCATION_PARAMETERS] =
        (_a = safelyJSONStringify(runExtra.invocation_params)) !== null && _a !== void 0 ? _a : undefined;
    if ((0, typeUtils_1.isString)(runExtra.invocation_params.model_name)) {
        openInferenceParams[openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME] =
            runExtra.invocation_params.model_name;
    }
    else if ((0, typeUtils_1.isString)(runExtra.invocation_params.model)) {
        openInferenceParams[openinference_semantic_conventions_1.SemanticConventions.LLM_MODEL_NAME] =
            runExtra.invocation_params.model;
    }
    return openInferenceParams;
}
function getTemplateFromSerialized(serialized) {
    if (!(0, typeUtils_1.isObject)(serialized) || !(0, typeUtils_1.isObject)(serialized.kwargs)) {
        return null;
    }
    const messages = serialized.kwargs.messages;
    if (!(0, typeUtils_1.isNonEmptyArray)(messages)) {
        return null;
    }
    const firstMessage = messages[0];
    if (!(0, typeUtils_1.isObject)(firstMessage) || !(0, typeUtils_1.isObject)(firstMessage.prompt)) {
        return null;
    }
    const template = firstMessage.prompt.template;
    if (!(0, typeUtils_1.isString)(template)) {
        return null;
    }
    return template;
}
const safelyGetTemplateFromSerialized = (0, openinference_core_1.withSafety)({
    fn: getTemplateFromSerialized,
});
/**
 * A best effort function to extract the prompt template from a langchain run.
 * @param run - The langchain run to extract the prompt template from
 * @returns The OpenInference attributes for the prompt template
 */
function formatPromptTemplate(run) {
    var _a, _b;
    if (run.run_type.toLowerCase() !== "prompt") {
        return null;
    }
    return {
        [openinference_semantic_conventions_1.SemanticConventions.PROMPT_TEMPLATE_VARIABLES]: (_a = safelyJSONStringify(run.inputs)) !== null && _a !== void 0 ? _a : undefined,
        [openinference_semantic_conventions_1.SemanticConventions.PROMPT_TEMPLATE_TEMPLATE]: (_b = safelyGetTemplateFromSerialized(run.serialized)) !== null && _b !== void 0 ? _b : undefined,
    };
}
function getTokenCount(maybeCount) {
    return (0, typeUtils_1.isNumber)(maybeCount) ? maybeCount : undefined;
}
/**
 * Formats the token counts of a langchain run into OpenInference attributes.
 * @param outputs - The outputs of a langchain run
 * @returns The OpenInference attributes for the token counts
 *
 * @see https://github.com/langchain-ai/langchainjs/blob/main/langchain-core/src/language_models/chat_models.ts#L403 for how token counts get added to outputs
 */
function formatTokenCounts(outputs) {
    if (!(0, typeUtils_1.isObject)(outputs)) {
        return null;
    }
    const firstGeneration = getFirstOutputGeneration(outputs);
    /**
     * Some community models have non standard output structures and show token counts in different places notable ChatBedrock
     * @see https://github.com/langchain-ai/langchainjs/blob/a173e300ef9ada416220876a2739e024b3a7f268/libs/langchain-community/src/chat_models/bedrock/web.ts
     */
    // Generations is an array of arrays containing messages
    const maybeGenerationComponent = firstGeneration != null ? firstGeneration[0] : null;
    const maybeMessage = (0, typeUtils_1.isObject)(maybeGenerationComponent)
        ? maybeGenerationComponent.message
        : null;
    const usageMetadata = (0, typeUtils_1.isObject)(maybeMessage)
        ? maybeMessage.usage_metadata
        : null;
    if ((0, typeUtils_1.isObject)(usageMetadata)) {
        return {
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION]: getTokenCount(usageMetadata.output_tokens),
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT]: getTokenCount(usageMetadata.input_tokens),
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_TOTAL]: getTokenCount(usageMetadata.total_tokens),
        };
    }
    const llmOutput = outputs.llmOutput;
    if (!(0, typeUtils_1.isObject)(llmOutput)) {
        return null;
    }
    if ((0, typeUtils_1.isObject)(llmOutput.tokenUsage)) {
        return {
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION]: getTokenCount(llmOutput.tokenUsage.completionTokens),
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT]: getTokenCount(llmOutput.tokenUsage.promptTokens),
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_TOTAL]: getTokenCount(llmOutput.tokenUsage.totalTokens),
        };
    }
    /**
     * In the case of streamed outputs, the token counts are not available
     * only estimated counts provided by langchain (not the model provider) are available
     */
    if ((0, typeUtils_1.isObject)(llmOutput.estimatedTokenUsage)) {
        return {
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION]: getTokenCount(llmOutput.estimatedTokenUsage.completionTokens),
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT]: getTokenCount(llmOutput.estimatedTokenUsage.promptTokens),
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_TOTAL]: getTokenCount(llmOutput.estimatedTokenUsage.totalTokens),
        };
    }
    /**
     * In some cases community models have a different output structure do to the way they extend the base model
     * Notably ChatBedrock may have tokens stored in this format instead of normalized
     * @see https://github.com/langchain-ai/langchainjs/blob/a173e300ef9ada416220876a2739e024b3a7f268/libs/langchain-community/src/chat_models/bedrock/web.ts for ChatBedrock
     * and
     * @see https://github.com/langchain-ai/langchainjs/blob/main/langchain-core/src/language_models/chat_models.ts#L403 for nomalization
     */
    if ((0, typeUtils_1.isObject)(llmOutput.usage)) {
        const maybePromptTokens = getTokenCount(llmOutput.usage.input_tokens);
        const maybeCompletionTokens = getTokenCount(llmOutput.usage.output_tokens);
        let maybeTotalTokens = getTokenCount(llmOutput.usage.total_tokens);
        if (maybeTotalTokens == null) {
            maybeTotalTokens =
                (0, typeUtils_1.isNumber)(maybePromptTokens) && (0, typeUtils_1.isNumber)(maybeCompletionTokens)
                    ? maybePromptTokens + maybeCompletionTokens
                    : undefined;
        }
        return {
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_COMPLETION]: getTokenCount(maybeCompletionTokens),
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_PROMPT]: getTokenCount(maybePromptTokens),
            [openinference_semantic_conventions_1.SemanticConventions.LLM_TOKEN_COUNT_TOTAL]: maybeTotalTokens,
        };
    }
    return null;
}
/**
 * Formats the function calls of a langchain run into OpenInference attributes.
 * @param outputs - The outputs of a langchain run
 * @returns The OpenInference attributes for the function calls
 */
function formatFunctionCalls(outputs) {
    const firstGeneration = getFirstOutputGeneration(outputs);
    if (firstGeneration == null) {
        return null;
    }
    const maybeGeneration = firstGeneration[0];
    if (!(0, typeUtils_1.isObject)(maybeGeneration) || !(0, typeUtils_1.isObject)(maybeGeneration.message)) {
        return null;
    }
    const additionalKwargs = maybeGeneration.message.additional_kwargs;
    if (!(0, typeUtils_1.isObject)(additionalKwargs) ||
        !(0, typeUtils_1.isObject)(additionalKwargs.function_call)) {
        return null;
    }
    return {
        [openinference_semantic_conventions_1.SemanticConventions.LLM_FUNCTION_CALL]: safelyJSONStringify(additionalKwargs.function_call),
    };
}
/**
 * Formats the tool calls of a langchain run into OpenInference attributes.
 * @param run - The langchain run to extract the tool calls from
 * @returns The OpenInference attributes for the tool calls
 */
function formatToolCalls(run) {
    const normalizedRunType = run.run_type.toLowerCase();
    if (normalizedRunType !== "tool") {
        return null;
    }
    const toolAttributes = {
        [openinference_semantic_conventions_1.SemanticConventions.TOOL_NAME]: run.name,
    };
    if (!(0, typeUtils_1.isObject)(run.serialized)) {
        return toolAttributes;
    }
    if ((0, typeUtils_1.isString)(run.serialized.name)) {
        toolAttributes[openinference_semantic_conventions_1.SemanticConventions.TOOL_NAME] = run.serialized.name;
    }
    if ((0, typeUtils_1.isString)(run.serialized.description)) {
        toolAttributes[openinference_semantic_conventions_1.SemanticConventions.TOOL_DESCRIPTION] =
            run.serialized.description;
    }
    return toolAttributes;
}
/**
 * Formats the metadata of a langchain run into OpenInference attributes.
 * @param run - The langchain run to extract the metadata from
 * @returns The OpenInference attributes for the metadata
 */
function formatMetadata(run) {
    if (!(0, typeUtils_1.isObject)(run.extra) || !(0, typeUtils_1.isObject)(run.extra.metadata)) {
        return null;
    }
    return {
        [openinference_semantic_conventions_1.SemanticConventions.METADATA]: safelyJSONStringify(run.extra.metadata),
    };
}
/**
 * Formats the session id of a langchain run into OpenInference attributes.
 *
 * @see https://docs.smith.langchain.com/observability/how_to_guides/monitoring/threads#group-traces-into-threads
 *
 * @param run - The langchain run to extract the session id from
 * @returns The OpenInference attributes for the session id
 */
function formatSessionId(run) {
    if (!(0, typeUtils_1.isObject)(run.extra)) {
        return null;
    }
    const metadata = run.extra.metadata;
    if (!(0, typeUtils_1.isObject)(metadata)) {
        return null;
    }
    const sessionId = exports.SESSION_ID_KEYS.find((key) => (0, typeUtils_1.isString)(metadata[key]));
    if (sessionId == null) {
        return null;
    }
    return {
        [openinference_semantic_conventions_1.SemanticConventions.SESSION_ID]: metadata[sessionId],
    };
}
exports.safelyFlattenAttributes = (0, openinference_core_1.withSafety)({
    fn: flattenAttributes,
    onError: onError("Error flattening attributes"),
});
exports.safelyFormatIO = (0, openinference_core_1.withSafety)({
    fn: formatIO,
    onError: onError("Error formatting IO"),
});
exports.safelyFormatInputMessages = (0, openinference_core_1.withSafety)({
    fn: formatInputMessages,
    onError: onError("Error formatting input messages"),
});
exports.safelyFormatOutputMessages = (0, openinference_core_1.withSafety)({
    fn: formatOutputMessages,
    onError: onError("Error formatting output messages"),
});
exports.safelyGetOpenInferenceSpanKindFromRunType = (0, openinference_core_1.withSafety)({
    fn: getOpenInferenceSpanKindFromRunType,
    onError: onError("Error getting OpenInference span kind from run type"),
});
exports.safelyFormatRetrievalDocuments = (0, openinference_core_1.withSafety)({
    fn: formatRetrievalDocuments,
    onError: onError("Error formatting retrieval documents"),
});
exports.safelyFormatLLMParams = (0, openinference_core_1.withSafety)({
    fn: formatLLMParams,
    onError: onError("Error formatting LLM params"),
});
exports.safelyFormatPromptTemplate = (0, openinference_core_1.withSafety)({
    fn: formatPromptTemplate,
    onError: onError("Error formatting prompt template"),
});
exports.safelyFormatTokenCounts = (0, openinference_core_1.withSafety)({
    fn: formatTokenCounts,
    onError: onError("Error formatting token counts"),
});
exports.safelyFormatFunctionCalls = (0, openinference_core_1.withSafety)({
    fn: formatFunctionCalls,
    onError: onError("Error formatting function calls"),
});
exports.safelyFormatToolCalls = (0, openinference_core_1.withSafety)({
    fn: formatToolCalls,
    onError: onError("Error formatting tool calls"),
});
exports.safelyFormatMetadata = (0, openinference_core_1.withSafety)({
    fn: formatMetadata,
    onError: onError("Error formatting metadata"),
});
exports.safelyFormatSessionId = (0, openinference_core_1.withSafety)({
    fn: formatSessionId,
    onError: onError("Error formatting session id"),
});
//# sourceMappingURL=utils.js.map